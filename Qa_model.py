# -*- coding: utf-8 -*-
"""Copy of QA_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1983zavow8jq1CEUhocwMfmDFrP-aKcSf
"""

import pandas as pd
import numpy as np
import string
from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.corpus import wordnet
from nltk import pos_tag
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import TruncatedSVD
from collections import OrderedDict

df1 = pd.read_csv('./dataset/S08_question_answer_pairs.txt', sep='\t')
df2 = pd.read_csv('./dataset/S09_question_answer_pairs.txt', sep='\t')
df3 = pd.read_csv('./dataset/S10_question_answer_pairs.txt', sep='\t', encoding = 'ISO-8859-1')

data = df1.append([df2,df3])
data.info()

data = data.dropna()
data = data.drop_duplicates(subset='Question')
data.shape

data['Answer'] = data['Answer'].apply(lambda x: x.lower())
data['Answer'] = data['Answer'].str.strip(".")
data['Question'] = data['Question'].str.strip()
data.head()

data['ArticleTitle'] = data['ArticleTitle'].str.replace('_', ' ')
data['QuestionOriginal'] = data['Question']
data['Question'] = data['ArticleTitle'] + " " + data['Question'] + " " + data['Answer'] #include article title and answer for more information
data = data[['QuestionOriginal','Question','Answer']]
data.head()

"""# Set up the BOW Algorithm

We need to next pre-process our data so that it's in a useable format by the ML algorithm. This involves 3 main steps: tokenizing, removing stop words, and lemmatizing. This function below my_tokenizer() handles the removal of stop words and the lemmatizing.

"""

import nltk
# nltk.download('stopwords')

stopwords_list = set(stopwords.words('english'))

lemmatizer = WordNetLemmatizer()

def my_tokenizer(doc):
    doc = doc.lower()
    words = word_tokenize(doc)
    pos_tags = pos_tag(words)
    
    non_stopwords = [w for w in pos_tags if not w[0].lower() in stopwords_list]
    
    non_punctuation = [w for w in non_stopwords if not w[0] in string.punctuation]
     
    lemmas_temp = []
    lemmas = []
    for w in non_punctuation:
        if w[1].startswith('J'):
            pos = wordnet.ADJ
        elif w[1].startswith('V'):
            pos = wordnet.VERB
        elif w[1].startswith('N'):
            pos = wordnet.NOUN
        elif w[1].startswith('R'):
            pos = wordnet.ADV
        else:
            pos = wordnet.NOUN
        
        lemmas_temp.append(lemmatizer.lemmatize(w[0], pos))
   
    for item in lemmas_temp:
        remove_repeats = (' '.join(OrderedDict.fromkeys(item.split())))
        lemmas.append(remove_repeats)

    return lemmas

"""This code here handles the vectorization step, which converts our text to integer representation. We're also using SVD at this stage to reduce the components in our resulting matrix and thereby improve the performance of our model.

"""

import nltk
# nltk.download('punkt')
# nltk.download('averaged_perceptron_tagger')
# nltk.download('wordnet')

tfidf_vectorizer = TfidfVectorizer(tokenizer=my_tokenizer)
tfidf_matrix = tfidf_vectorizer.fit_transform(tuple(data['Question']))

svd_transformer = TruncatedSVD(n_components=1000)
svd_transformer.fit(tfidf_matrix)

"""We calculate the ideal number of SVD variables to use on the data by adding up the cumulative variance till it's 80%. This cap at 80% prevents overfitting of the model"""

k = 0
cumul_var=0.0

for var in sorted(svd_transformer.explained_variance_ratio_):
    cumul_var += var
    if cumul_var >= 0.8:
        break
    else:
        k+=1
print(k)

svd_transformer  = TruncatedSVD(n_components=k)
svd_data = svd_transformer.fit(tfidf_matrix)

tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(tuple(data['Question']))

"""# The actual predictions

We use the cosine similarity function to compare the user's input to our matrix of known document embeddings we calculated above. Query_vect is the user's input, once it has been pre-processed according to the rules we discussed above.
"""

def general(question):
    query_vect = tfidf_vectorizer.transform([question])
    similarity = cosine_similarity(query_vect, tfidf_matrix)
    index_similarity = np.argmax(similarity, axis=None)
    
    return similarity, index_similarity

"""This is our driver function that takes in the user's question input, processes it, and outputs the result. Notice how the QuestionOriginal column permits us to output the original question without the ArticleTitle appended, making it more natural for the user to interpret the results. We found that 0.3 is a good threshold that permits relevant and correct answers to be reported while filtering out other question responses for which we simple don't have enough training data to provide a meaningful output."""

def ask(question):
    print('Your question:', question)
    
    similarity, index_similarity = general(question) 
    answer = data.iloc[index_similarity]['Answer']
    
    if answer == 'no':
        print("It's a No thing")
        
    elif answer == 'yes':
        print("It's a Yes thing")

    else:
        print("It's not a Yes/No thing")
            
    if ((similarity[0, index_similarity])>0):
        print('Closest question found:', data.iloc[index_similarity]['QuestionOriginal'])
        print('Similarity: {:.2%}'.format(similarity[0, index_similarity]))
        print('Answer:', data.iloc[index_similarity]['Answer'])
    else:
        print("Not really any great matches for that, sorry")

"""Here's where you can test this model out on some questions. """
print("welcome")
ques = input("type in your question")
ask(ques)
ask('Was Abraham Lincoln the sixteenth President of the United States?')
